# vLLM Configuration for Llama-3.1-8B with Speculative Decoding
# Based on the launch.json "vLLM Server (Spec Decode, 3.1-8B)" configuration

# Server Configuration
host: "0.0.0.0"
port: 8000

# Model Configuration
model: "meta-llama/Llama-3.1-8B-Instruct"

# Performance and Optimization
# enforce_eager: true

# Speculative Decoding Configuration (as JSON)
speculative_config: '{"method": "draft_model", "model": "meta-llama/Llama-3.2-1B-Instruct", "num_speculative_tokens": 6}'

enable_prefix_caching: false
# Environment Variables (these would need to be set externally)
# VLLM_SERVER_DEV_MODE: "1"
# VLLM_USE_V1: "1" 
# VLLM_ENABLE_V1_MULTIPROCESSING: "0"
# CUDA_VISIBLE_DEVICES: "0"
